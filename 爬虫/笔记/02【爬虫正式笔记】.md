# 一. urllib

```python
# urllib 的基本使用
# 使用urllib来获取百度首页的源码
import urllib.request

# (1)定义一个url  就是你要访问的地址
url = 'http://www.baidu.com/'

# (2)模拟浏览器向服务器发送请求 response响应
response = urllib.request.urlopen(url)

# （3）获取响应中的页面的源码  content 内容的意思
# read方法  返回的是字节形式的二进制数据
# 我们要将二进制的数据转换为字符串
# 二进制--》字符串  解码  decode('编码的格式')
content = response.read().decode('utf-8')

# （4）打印数据
print(content)
```

```python
# 1 个类型和6个方法
import urllib.request

url = 'http://www.baidu.com'

# 模拟浏览器向服务器发送请求
response = urllib.request.urlopen(url)

# 一个类型和六个方法
# response是HTTPResponse的类型
# print(type(response))

# 按照一个字节一个字节的去读
# content = response.read()
# print(content)

# 返回多少个字节  这里返回 5 个字节
# content = response.read(5)
# print(content)

# 读取一行
# content = response.readline()
# print(content)
# 读取多行
# content = response.readlines()
# print(content)

# 返回状态码  如果是200了 那么就证明我们的逻辑没有错
# print(response.getcode())

# 返回的是url地址
# print(response.geturl())

# 获取是一个状态信息
print(response.getheaders())

# 一个类型 HTTPResponse
# 六个方法 read  readline  readlines  getcode geturl getheaders
```

| 类型与方法   | 说明                                    |
| ------------ | --------------------------------------- |
| HTTPResponse | response是HTTPResponse的类型            |
| read         | 一个字节字节地去读                      |
| readline     | 一行读取                                |
| readlines    | 多行读取                                |
| getcode      | 获取状态码，如果是200，表示连接没有问题 |
| geturl       | 获取当前访问的网址                      |
| getheaders   | 获取状态信息                            |



```python
import urllib.request

# 下载网页
# url_page = 'http://www.baidu.com'

# url代表的是下载的路径  filename文件的名字
# 在python中 可以变量的名字  也可以直接写值
# urllib.request.urlretrieve(url_page,'baidu.html')

# 下载图片
# url_img = 'https://img1.baidu.com/it/u=3004965690,4089234593&fm=26&fmt=auto&gp=0.jpg'
#
# urllib.request.urlretrieve(url= url_img,filename='lisa.jpg')

# 下载视频
url_video = 'https://vd3.bdstatic.com/mdamhkku4ndaka5etk3/1080p/cae_h264/1629557146541497769/mda-mhkku4ndaka5etk3.mp4?v_from_s=hkapp-haokan-tucheng&auth_key=1629687514-0-0-7ed57ed7d1168bb1f06d18a4ea214300&bcevod_channel=searchbox_feed&pd=1&pt=3&abtest='

urllib.request.urlretrieve(url_video,'hxekyyds.mp4')
```

## 1.1 **请求对象的定制** 

```python
UA介绍：User Agent中文名为用户代理，简称 UA，它是一个特殊字符串头，使得服务器能够识别客户使用的操作系统 及版本、CPU 类型、浏览器及版本。浏览器内核、浏览器渲染引擎、浏览器语言、浏览器插件等
```

```python
语法：request = urllib.request.Request()
```

扩展：编码的由来 

编码集的演变‐‐‐ 

由于计算机是美国人发明的，因此，最早只有127个字符被编码到计算机里，也就是大小写英文字母、数字和一些符号， 这个编码表被称为ASCII编码，比如大写字母A的编码是65，小写字母z的编码是122。 但是要处理中文显然一个字节是不够的，至少需要两个字节，而且还不能和ASCII编码冲突， 所以，中国制定了GB2312编码，用来把中文编进去。 你可以想得到的是，全世界有上百种语言，日本把日文编到Shift_JIS里，韩国把韩文编到Euc‐kr里， 各国有各国的标准，就会不可避免地出现冲突，结果就是，在多语言混合的文本中，显示出来会有乱码。 因此，Unicode应运而生。Unicode把所有语言都统一到一套编码里，这样就不会再有乱码问题了。 

Unicode标准也在不断发展，但最常用的是用两个字节表示一个字符（如果要用到非常偏僻的字符，就需要4个字节）。 现代操作系统和大多数编程语言都直接支持Unicode。

## 1.2 **编解**码

1. **get  请求方式：`urllib.parse.quote（）`**

```python
import urllib.request 
import urllib.parse 

url = 'https://www.baidu.com/s?wd=' 
headers = { 'User‐Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36' }

url = url + urllib.parse.quote('小野') 
request = urllib.request.Request(url=url,headers=headers) 
response = urllib.request.urlopen(request) 
print(response.read().decode('utf‐8'))
```

2. **get请求方式：`urllib.parse.urlencode（）`**

```python
import urllib.request 
import urllib.parse 

url = 'http://www.baidu.com/s?' 
data = { 'name':'小刚', 'sex':'男', }
data = urllib.parse.urlencode(data) 
url = url + data 
print(url) 

headers = { 'User‐Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36' }

request = urllib.request.Request(url=url,headers=headers)
response = urllib.request.urlopen(request) 
print(response.read().decode('utf‐8'))
```

**3.post请求方式** 

```python
eg:百度翻译 
import urllib.request 
import urllib.parse 

url = 'https://fanyi.baidu.com/sug' 
headers = { 'user‐agent': 'Mozilla/5.0 (Windows NT 10.0; Win64;x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36' }

keyword = input('请输入您要查询的单词') 
data = { 'kw':keyword }
data = urllib.parse.urlencode(data).encode('utf‐8') 
request = urllib.request.Request(url=url,headers=headers,data=data) 
response = urllib.request.urlopen(request) 
content = response.read().decode('utf‐8')

import json
obj = json.loads(content)
print(obj)

# 下面是跟着视频敲得代码
import urllib.parse
import urllib.request
import urllib.response

url = 'https://fanyi.baidu.com/sug'

headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.131 Safari/537.36 SLBrowser/8.0.0.9071 SLBChan/30'}

data = {'kw':'spider'}

# post 请求的参数，必须要进行编码
data = urllib.parse.urlencode(data).encode('utf-8')
#print(data)

# post的请求参数，是不会拼接在url 的后面的，而是需要放在对象定制的参数中
# post 请求的参数，必须要进行编码
request = urllib.request.Request(url=url,data=data,headers=headers)
# sprint(request)

# 模拟浏览器服务器发送请求
response = urllib.request.urlopen(request)
#print(response)

# 获取响应数据
content = response.read().decode('utf-8')
print(content)

import json
obj = json.loads(content)
print(obj)

# post请求方式的参数 必须编码   
data = urllib.parse.urlencode(data)
# 编码之后 必须调用encode方法 
data = urllib.parse.urlencode(data).encode('utf-8')
# 参数是放在请求对象定制的方法中  
request = urllib.request.Request(url=url,data=data,headers=headers)
```

> 
>
> 总结：post和get区别？ 
>
> 1：get请求方式的参数必须编码，参数是拼接到url后面，编码之后不需要调用encode方法 
>
> 2：post请求方式的参数必须编码，参数是放在请求对象定制的方法中，编码之后需要调用encode方法 



```python
import urllib.request

# get请求
# 获取豆瓣电影的第一页的数据 并且保存起来

import urllib.request

url = 'https://movie.douban.com/j/chart/top_list?type=5&interval_id=100%3A90&action=&start=0&limit=20'

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'
}

# (1) 请求对象的定制
request = urllib.request.Request(url=url,headers=headers)

# （2）获取响应的数据
response = urllib.request.urlopen(request)
content = response.read().decode('utf-8')

# (3) 数据下载到本地
# open方法默认情况下使用的是gbk的编码  如果我们要想保存汉字 那么需要在open方法中指定编码格式为utf-8
# encoding = 'utf-8'
# fp = open('douban.json','w',encoding='utf-8')
# fp.write(content)

with open('douban1.json','w',encoding='utf-8') as fp:
    fp.write(content)
```

```python
# 下载豆瓣前10页内容
# https://movie.douban.com/j/chart/top_list?type=5&interval_id=100%3A90&action=&
# start=0&limit=20

# https://movie.douban.com/j/chart/top_list?type=5&interval_id=100%3A90&action=&
# start=20&limit=20

# https://movie.douban.com/j/chart/top_list?type=5&interval_id=100%3A90&action=&
# start=40&limit=20

# https://movie.douban.com/j/chart/top_list?type=5&interval_id=100%3A90&action=&
# start=60&limit=20

# page    1  2   3   4
# start   0  20  40  60

# start （page - 1）*20


# 下载豆瓣电影前10页的数据
# （1） 请求对象的定制
# （2） 获取响应的数据
# （3） 下载数据

import urllib.parse
import urllib.request

def create_request(page):
    base_url = 'https://movie.douban.com/j/chart/top_list?type=5&interval_id=100%3A90&action=&'

    data = {
        'start':(page - 1) * 20,
        'limit':20
    }

    data = urllib.parse.urlencode(data)

    url = base_url + data

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'
    }

    request = urllib.request.Request(url=url,headers=headers)
    return request


def get_content(request):
    response = urllib.request.urlopen(request)
    content = response.read().decode('utf-8')
    return content


def down_load(page,content):
    with open('douban_' + str(page) + '.json','w',encoding='utf-8')as fp:
        fp.write(content)




# 程序的入口
if __name__ == '__main__':
    start_page = int(input('请输入起始的页码'))
    end_page = int(input('请输入结束的页面'))

    for page in range(start_page,end_page+1):
#         每一页都有自己的请求对象的定制
        request = create_request(page)
#         获取响应的数据
        content = get_content(request)
#         下载
        down_load(page,content)
```

## 1.3 **Handler 处理器**

> 为什么要学习handler？ 
>
> urllib.request.urlopen(url) 
>
> 不能定制请求头 
>
> urllib.request.Request(url,headers,data) 
>
> 可以定制请求头 
>
> Handler 
>
> 定制更高级的请求头（随着业务逻辑的复杂 请求对象的定制已经满足不了我们的需求（动态cookie和代理 
>
> 不能使用请求对象的定制） 



```python
import urllib.request 

url = 'http://www.baidu.com' 
headers = { 'User ‐ Agent': 'Mozilla / 5.0(Windows NT 10.0;Win64;x64) AppleWebKit / 537.36(KHTML, likeGecko) Chrome / 74.0.3729.169Safari / 537.36' }

request = urllib.request.Request(url=url,headers=headers)
#（1）获取hanlder对象
handler = urllib.request.HTTPHandler() 
#（2）获取opener对象
opener = urllib.request.build_opener(handler) 
#(3) 调用open方法
response = opener.open(request) 

content = response.read().decode('utf-8')
print(response.read().decode('utf‐8'))
```

## 1.4 **代理服务器**

```python
1.代理的常用功能? 
	1.突破自身IP访问限制，访问国外站点。 
	2.访问一些单位或团体内部资源 扩展：某大学FTP(前提是该代理地址在该资源的允许访问范围之内)，使用教育	    网内地址段免费代理服务器，就可以用于对教育网开放的各类FTP下载上传，以及各类资料查询共享等服          务。
	3.提高访问速度 扩展：通常代理服务器都设置一个较大的硬盘缓冲区，当有外界的信息通过时，同时也将其保存		 到缓冲 区中，当其他用户再访问相同的信息时， 则直接由缓冲区中取出信息，传给用户，以提高访问速度。
	4.隐藏真实IP 扩展：上网者也可以通过这种方法隐藏自己的IP，免受攻击。 

2.  代码配置代理 
	创建Reuqest对象 
	创建ProxyHandler对象 
	用handler对象创建opener对象 
	使用opener.open函数发送请求
```

```python
import urllib.request 

url = 'http://www.baidu.com/s?wd=ip' 
headers = { 'User ‐ Agent': 'Mozilla / 5.0(Windows NT 10.0;Win64;x64) AppleWebKit / 537.36(KHTML, likeGecko) Chrome / 74.0.3729.169Safari / 537.36' }

request = urllib.request.Request(url=url,headers=headers) 
proxies = {'http':'117.141.155.244:53281'} 

handler = urllib.request.ProxyHandler(proxies=proxies) 

opener = urllib.request.build_opener(handler) 
response = opener.open(request) 
content = response.read().decode('utf‐8') 

with open('daili.html','w',encoding='utf‐8')as fp:
    fp.write(content)
```

# 二. **解析**

## **2.1 .xpath** 

```python
xpath使用： 
注意：提前安装xpath插件 
（1）打开chrome浏览器 
（2）点击右上角小圆点 
（3）更多工具 
（4）扩展程序 
（5）拖拽xpath插件到扩展程序中 
（6）如果crx文件失效，需要将后缀修改zip 
（7）再次拖拽 
（8）关闭浏览器重新打开 
（9）ctrl + shift + x
（10）出现小黑框 
1.安装lxml库pip install lxml ‐i https://pypi.douban.com/simple 
2.导入lxml.etree from lxml import etree 
3.etree.parse() 解析本地文件
	html_tree = etree.parse('XX.html') 
4.etree.HTML() 服务器响应文件 
	html_tree = etree.HTML(response.read().decode('utf‐8') 
5.html_tree.xpath(xpath路径)
```

```python
xpath基本语法：
1.路径查询 //：查找所有子孙节点，不考虑层级关系 
	      / ：找直接子节点 
2.谓词查询 //div[@id] 
		  //div[@id="maincontent"] 
3.属性查询 //@class 
4.模糊查询 //div[contains(@id, "he")] 
		  //div[starts‐with(@id, "he")]
5.内容查询 //div/h1/text() 
6.逻辑运算 并：  //div[@id="head" and @class="s_down"] 
		  或： //title |   
         `tree.xpath('//ul/li[@id="c3"]/text() | //ul/li[@id="c4"]/text()')`
          //price
```

应用案例： 1.站长素材图片抓取并且下载（http://sc.chinaz.com/tupian/shuaigetupian.html）--》懒加载 

==自己安装的建议：直接在浏览器里面输入’’xpath helper’，直接安装（限谷歌浏览器）’==



## **2.2 JsonPath** 

```python
jsonpath的安装及使用方式： 
	pip安装：
	pip install jsonpath 
jsonpath的使用： 
	obj = json.load(open('json文件', 'r', encoding='utf‐8')) 
	ret = jsonpath.jsonpath(obj, 'jsonpath语法')
```

## **2.3 BeautifulSoup** 

### 1.基本简介

- BeautifulSoup简称： bs4 

- 什么是BeatifulSoup？ 
  - BeautifulSoup，和lxml一样，是一个html的解析器，主要功能也是解析和提取数据 

- 优缺点？ 

  - 缺点：效率没有lxml的效率高 

  - 优点：接口设计人性化，使用方便 

### 2.安装以及创建

```python
1.安装pip install bs4 
2.导入from bs4 import BeautifulSoup 
3.创建对象 服务器响应的文件生成对象 
	soup = BeautifulSoup(response.read().decode(), 'lxml') 
本地文件生成对象 soup = BeautifulSoup(open('1.html'),encoding="utf-8", 'lxml') 
注意：默认打开文件的编码格式gbk所以需要指定打开编码格式
```

### 3.节点定位 

```python
1.根据标签名查找节点 
soup.a 【注】只能找到第一个a 
soup.a.name 
soup.a.attrs 
2.函数 
(1).find(返回一个对象) 
find('a')：只找到第一个a标签
find('a', title='名字') 
find('a', class_='名字') 
(2).find_all(返回一个列表) 
find_all('a') 查找到所有的a 
find_all(['a', 'span']) 返回所有的a和span find_all('a', limit=2) 只找前两个a 
(3).select(根据选择器得到节点对象)【推荐】 
1.element eg:p 
2..class eg:.firstname 
3.#id
eg:#firstname 
4.属性选择器 
[attribute] 
eg:li = soup.select('li[class]') [attribute=value] 
eg:li = soup.select('li[class="hengheng1"]') 
5.层级选择器
element element 
div p 
element>element 
div>p 
element,element 
div,p 
eg:soup = soup.select('a,span') 

```

### 4 节点信息

```python
(1).获取节点内容：适用于标签中嵌套标签的结构 obj.string 
obj.get_text()【推荐】 
(2).节点的属性 
tag.name 获取标签名 
eg:tag = find('li) 
print(tag.name)
tag.attrs将属性值作为一个字典返回 
(3).获取节点属性 
obj.attrs.get('title')【常用】 obj.get('title') obj['title']
```

```
result = tree.xpath('//span[@class="title-"]/@value')[0]
```



通过解析本地文件 来将bs4的基础语法进行讲解，默认打开的文件的编码格式是gbk 所以在打开文件的时候需要指定编码

```python
soup = BeautifulSoup(open('075_尚硅谷_爬虫_解析_bs4的基本使用.html',encoding='utf-8'),'lxml')
```



```python
 #根据标签名查找节点
#找到的是第一个符合条件的数据
 print(soup.a)
# 获取标签的属性和属性值
 print(soup.a.attrs)
```

### 5 bs4的一些函数

#### （1）find

```python
# 返回的是第一个符合条件的数据
print(soup.find('a'))

# 根据title的值来找到对应的标签对象
print(soup.find('a',title="a2"))

# 根据class的值来找到对应的标签对象  注意的是class需要添加下划线
print(soup.find('a',class_="a1"))

```

#### （2）find_all  返回的是一个列表 并且返回了所有的a标签

```python
print(soup.find_all('a'))

# 如果想获取的是多个标签的数据 那么需要在find_all的参数中添加的是列表的数据
print(soup.find_all(['a','span']))

# limit的作用是查找前几个数据
print(soup.find_all('li',limit=2))
```

#### （3）select（推荐）

```python
# select方法返回的是一个列表  并且会返回多个数据
# print(soup.select('a'))

# 可以通过.代表class  我们把这种操作叫做类选择器
# print(soup.select('.a1'))

# print(soup.select('#l1'))


# 属性选择器---通过属性来寻找对应的标签
# 查找到li标签中有id的标签
print(soup.select('li[id]'))

# 查找到li标签中id为l2的标签
# print(soup.select('li[id="l2"]'))
```

### 6 层级选择器

#### 后代选择器

```
# 找到的是div下面的li
# print(soup.select('div li'))
```

#### 子代选择器

```
#  某标签的第一级子标签
# 注意：很多的计算机编程语言中 如果不加空格不会输出内容  但是在bs4中 不会报错 会显示内容
# print(soup.select('div > ul > li'))


# 找到a标签和li标签的所有的对象
# print(soup.select('a,li'))
```



# 三 文件的读写操作

## 3.1 读取整个文件

我们首先有个text01.txt，我们读取整个文件，看看里面有什么内容

```python
#读取整个文件
with open('text01.txt') as file_read:
    content=file_read.read()
    print(content)
```

结果如下：

![](https://img-blog.csdnimg.cn/b1594f41573f4a7a8e143ed87126e0c8.png)

我们先来看看函数open()。要想使用文件第一步都得先打开文件，这样才能访问它，我们看一下这些函数都起到什么作用：

函数open()接受一个参数： 要打开的文件的名称。Python在当前执行的文件所在的目录中查找指定的文件。在这个示例中，函数open() 返回一个表示文件的对象。在这里，open(‘text01.txt’)返回一个表示文件text01.txt的对象；Python将这个对象存储在我们将在后面使用的file_read变量中。
关键字with在不再需要访问文件后将其关闭。在这个程序中，注意到我们调用了open()，但没有调用close()；你也可以调用open()和close()来打开和关闭文件，如果有bug，导致close()语句未执行，文件将不会关闭。这样会导致文件损坏，如果过早调用close，将会导致程序bug，这会导致更多的错误。所以直接用with可以让程序自己去判断文件的关闭。
获取text01.txt的文件对象后，我们使用方法read()读取这个文件的全部内容，并将其作为一个长长的字符串存储在变量contents中，打印出来。

## 3.2 文件路径

一般来说，如果需要读取的文件和程序文件在一个文件目录下面，open()函数中直接写文件路径名即可，如果不在同一目录下，那么就需要确定读取文件的确定位置。
我的文件在该目录下：

读取文件时这样，需要注意的是文件地址用的是 ‘\’ 而不是 ‘/’ ，最重要的的是不要有中文目录，可能会报错，原因是因为中文的编码问题

```python
#绝对路径读取文件
with open('\Users\macos\pyproject\text01.txt') as file_read:
    content=file_read.read()
    print(content)
```

## 3.3 逐行读取

读取文件时，常常需要检查其中的每一行：你可能要在文件中查找特定的信息，或者要以某种方式修改文件中的文本。

```python
#逐行读取文件并输出
with open('text01.txt') as file_read:
    for line in file_read:
        print(line)
```


这里我们利用for循环将读取的内容逐行打印了出来。

![](https://img-blog.csdnimg.cn/ad3150c2f25a4fba933399b2c001e755.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5p-z5bCP6JGx,size_17,color_FFFFFF,t_70,g_se,x_16)

使用关键字with时，open()返回的文件对象只在with代码块内可用。如果要在with代码块外访问文件的内容，可在with代码块内将文件的各行存储在一个列表中，并在with代码块外使用该列表：你可以立即处理文件的各个部分，也可推迟到程序后面再处理。





```python
with open('text01.txt') as file_read:
    #将文件存在lines中
    lines = file_read.readlines()

#循环读取文件
for line in lines:
    print(line.rstrip())#rstrip除去空白
```


readlines()从文件中读取每一行，并将其存储在一个列表中；接下来，该列表被存储到变量lines中；在with代码块外，我们依然可以使用这个变量。for循环来打印lines中的各行。由于列表lines的每个元素都对应于文件中的一行，因此输出与文件内容完全一致。

结果如下：

![](https://img-blog.csdnimg.cn/385e6f7a1f6547fe8dd52f87a2eb5fce.png)

## 3.4 写入文件

读文件是使用文件的基础，那么写入文件就是保存文件的基础。

```python
#将文本写入到text02.txt文件
with open('text02.txt','w') as file_read:
    file_read.write('I love wulip')
```


结果如下：打开文件text02.txt

![](https://img-blog.csdnimg.cn/2d0e2732dd3240e59f968e957c29028f.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5p-z5bCP6JGx,size_20,color_FFFFFF,t_70,g_se,x_16)

调用open()时提供了两个实参。第一个实参也是要打开的文件的名称； 第二个实参（‘w’）告诉Python，以写入模式打开这个文件。打开文件时，可指定读取模 式（‘r’）、写入模式（‘w’）、附加模式（‘a’）或让你能够读取和写入文件的模式（‘r+’）。如果你省略了模式实参，Python将以默认的只读模式打开文件。
如果你要写入的文件不存在，函数open()将自动创建它。 如果文件已经存在，将会清空文件内容，写入新的内容。
方法write()将一个字符串写入文件。
Python只能将字符串写入文本文件。要将数值数据存储到文本文件中，必须先使用函数str()将其转换为字符串格式。

## 3.5 写入多行

写入多行需要注意的是使用换行符，如果不使用，将还是在一行。

```python
#不使用换行符
with open('text03.txt','w') as file_read:
    file_read.write('I love wulip')
    file_read.write('she is my apple')
```


结果都挤在一行了：

![](https://img-blog.csdnimg.cn/0059a1cfea4a45c8b4dcdb83c33f6440.png)


```python
使用换行符后
with open('text03.txt','w') as file_read:
    file_read.write('I love wulip \n')
    file_read.write('she is my apple \n')
```

结果如下：

![](https://img-blog.csdnimg.cn/ae25e55c32e04130a242b749fa809e68.png)

## 3.6 追加写入文件

上面我们说到‘w’模式是写入文件，会直接清除之前写入的内容，这里我们介绍一下怎么追加写入道文件中。

```python
#追加写入文件
with open('text03.txt','a') as file_read:
    file_read.write('wulip love me too \n')
    file_read.write('she is my baby \n')
```



结果如下：

![](https://img-blog.csdnimg.cn/a3492c2aad0f4f8a878021f8dae09f7c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5p-z5bCP6JGx,size_17,color_FFFFFF,t_70,g_se,x_16)


以附加模式 ‘a’ 打开文件时，Python不会在返回文件对象前清空文件，而你写入到文件的行都将添加到文件末尾。 如果指定的文件不存在，Python将为你创建一个空文件。


