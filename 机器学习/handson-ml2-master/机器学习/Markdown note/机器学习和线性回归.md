# 机器学习和线性回归

无论是机器学习，还是深度学习，都离不开神经网络（Neural Network），起源于人们对生物体神经的认知，生物神经网络是由神经元、突触等结构组成，大量的神经元通过无数的突触连接在一起构成一个更大规模的神经网络，能处理人的思维和记忆，神经元的结构如下图所示。

![](https://wx4.sinaimg.cn/large/008uF2zMgy1h5uqkz03q8j30lt0ay76q.jpg)



**监督学习**

​      监督学习是机器学习中最重要的一类方法，占据了机器学习算法中绝大多数。所谓监督学习是指：机器在有已知输入值xi和输出值y的经验数据(样本)的情况下开展的学习。当输出y的值集是有限集合时，该学习问题为分类。诸如输出值y=[0|1|2]，就认为该预测样本为0类或1类或2类。若y值是数值型的连续值，则该学习问题为回归。以下图为例，我们将基于该样本集建立一个二分类模型，将图中的猫和老鼠分类开来。

![](https://wx2.sinaimg.cn/large/008uF2zMgy1h5uqkzekfbj30kj08o40v.jpg)

​       此处的监督学习，就是在已知动物样本的属性xi=[胡须，体色、门齿…]、输出值y=[猫|鼠]的情况下，让机器利用某种算法建立输入量xi和输出变量y的函数关系的过程。当模型训练结束后，只要你告诉机器某动物的特征值xi，它就能较为准确告诉你此动物是猫还是鼠。

这种学习方法有以下几点特点：

- 训练的数据有标签（lable）。即训练样本的输出变量是样本的标签。
- 样本的特征和标签已知。简单说，在训练前就知道输入和输出的值。
- 学习的目的就是建立一个将输入准确映射到输出的模型。



**无监督学习**       

无监督学习就是指机器在学习过程中不受监督，学习模型不断自我认知和巩固，最后进行自我归纳来达到学习目的。在现实生活中，我们常常遇到这样的场景：对有些问题缺乏足够的先验知识，难以对问题的输出结果进行人工标注。或者即便可以标注，但成本太高。因此，我们希望计算机能代替完成此类工作，比如将所有的样本自动分为不同的类别，再由人工对这些类别进行标注，有可能发现世界中新的类别。



准备数据，数据量越大越好，要构成特征和标签对。如要识别猫，就要有大量猫的图片和这个图片是猫的标签，构成特征标签对。随后，搭建神经网络的网络结构，并通过反向传播，优化连线的权重，直到模型的识别准确率达到要求，得到最优的连线权重，把这个模型保存起来。最后，用保存的模型，输入从未见过的新数据，它会通过前向传播，输出概率值，概率值最大的一个，就是分类或预测的结果。图2.1展示了搭建与使用神经网络模型的流程。

![](https://wx2.sinaimg.cn/large/008uF2zMgy1h5uqkyfbdhj30tb0aydiy.jpg)



无监督学习相对监督学习而言，有以下特点：

- 无需大量的标注数据，从而减少大量的人力、物力和财力
- 以更加接近人类的学习方式不断自我发现、学习和调整，有利于发现世界的内在结果，找到新的模式或新的知识。

常见的无监督学习算法有聚类、关联分析等



## 线性回归的数学表达式

​	线性回归（linear regression）是一种通过拟合自变量 x1 与因变量 y 之间最佳线性关系，来预测目标变量的方法。所谓的线性关系就是单个 x1 与 y 两者之间的关系可用一条直线近似表示，线性回归数学表达式为：



 y = p0 + p1x1 + p2x2 + p3x3 + ... +pnxn

 如果左式中只包括一个自变量x和一个因变量y，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。如果回归分析中包括两个或两个以上的自变量x，且因变量y和自变量x之间是线性关系，则称为多元线性回归分析。

在线性回归解决问题中，xi称为问题的特征值，如我们研究人的身高h与营养摄入量a、锻炼强度b和睡眠时长c的关系，其中a,b,c就是特征值，也叫自变量，h是因变量，我们试图找出一个类似式1的线性函数表达式，通过该函数作为模型，在经验数据的训练下，估计出该模型的回归参数β0β1β2…，那么该模型就可以根据目标特征性[x1,x2,…,xn]预测出目标的输出值y，从而达到利用线性模型解决预测问题或计算趋势值的目的。



### 线性回归的几个概念

 如前所述，回归求解过程就是用线性函数去拟合样本集，使所有样本与拟合函数之间的误差最小。如下图所示的红色圆点是一家贸易公司的广告投入与销售额之间的散点图，黑色的直线就是我们要拟合的一条直线，目标是找到一条放在合理位置的直线，所有圆点到直线的距离的平方和最小。

![](https://wx2.sinaimg.cn/large/008uF2zMgy1h5uqkzuso5j30ja0cvgmi.jpg)

 此处找直线的问题其实就是求函数 y=β0+β1x 中的系数 β0、β1，在不断寻找（β0，β1）的过程中，必须有一种判断标准来决定何时停止迭代寻找。在线性回归中，通常使用一种称为普通最小二乘（Ordinary Least Squares,OLS）的估计方法,使得y的预测值    和真实值之间的垂直距离的平方和最小。用下式表示：

![](https://wx2.sinaimg.cn/large/008uF2zMgy1h5uql09ux0j30a804xt8s.jpg)

​		SSE表示一种总的误差，也叫损失函数，它一定程度上反应了回归方程以外因素的影响，从而导致有一定的误差，这种误差是无法避免的。此外，拟合也与样本的质量有关，试想一下，如果上张散点图的圆点本身分布波动太大，说明个别样本的变化有些偶然因素，**样本的整体质量不高，从而也会导致拟合误差**。样本的波动用下式表示：

![](https://wx4.sinaimg.cn/large/008uF2zMgy1h5uql0n18xj308203c3yg.jpg)

SST表示了样本中所有因变量y的实际值与其平均值 y`一把`的差的平方和，其值越大，说明原始的样本数据本身具有较大的波动，反应了因变量的整体偏差。那如何评价求出的回归模型的好坏程度呢？在统计学中用下列式子来判断回归方程的拟合程度。

![](https://wx2.sinaimg.cn/large/008uF2zMgy1h5uql12596j306i030q2v.jpg)

R2称为判断系数或拟合优度。由上式可知，当回归方程以外因素引起的误差SSE越小时，R2就越接近1，表示此回归方程可以很好解释因变量的变化；反之，如果SSE越大，接近总体偏差SST,那么R2就接近0，说明此问题可能不适合采用线性模型解决，因为因变量与自变量此时并不存在线性关系。



### 梯度下降法

 机器学习的目标就是最小化损失函数的值，考虑最简单的情况，假设线性方程为：y=b+wx ,则损失函数 L 可以理解为变量 b 和 w 的函数，记为 L(b,w)，此处的 b、w 也可以推广为一个向量，如 b=(β0),w=(β1,β2,…,βn) 为自变量的各系数。因为损失函数是一个凸函数，所以寻找损失函数 L(b,w) 的最小值的过程，实际就是按照某种方向，不断去微调b和w的值，一步一步尝试找到这个最小值。正如右图所示，一个人站在山顶的人，他试图努力花最短的距离到达谷底。显然，如果他每次都能迎着山坡最陡的方向往山坡下降的地方走，并不时地调整自己下山的步长，就能以最短的时间到达谷底。

![](https://wx1.sinaimg.cn/large/008uF2zMgy1h5uql1f9jzj30bo06tmxm.jpg)

 由此可见，这个人在下到谷底的过程中，需要不断做两件事：一是始终沿最陡峭的方向下山，二是根据下降的速度来调整步长，刚开始时，可以将步长迈大些，当逐渐接近目标时，则可适当减小步长。其中，最陡峭的方向其实就是某处的梯度方向，因为梯度表示某一函数在该点处的变化率最大，对单变量的实值函数，梯度就是导数，而对于一个线性函数，梯度就是直线的斜率，步长又称为学习率    ，所以，我们把上述沿梯度方向逐步寻找损失函数最小值的方法称为梯度下降法。



损失函数最终都可以收敛到最小值或接近最小值。由上式不难看出，学习率 α 决定了 wi 变化的幅度，它决定了梯度下降的速率和稳定性，其大小需要由外界指定，不能由机器学习自身学习得来。如果 α 取值太小，就像人的步长太短一样，到达谷底就要花很长时间，导致学习效率低；如果 α 取值太大，虽然学习效率高，就像人的步距大一样，能很快接近谷底，**但最后可能这个人一步跨过了谷底**，导致始终难以到达谷底，也就是始终找不到损耗函数的最小值。因此，通常的做法是下图所示。



![](https://wx4.sinaimg.cn/large/008uF2zMgy1h5uql2ev9bj30a608edg1.jpg)

 在沿着梯度方向寻找损耗函数最小值的过程中，为了平衡大小步距的优缺点，可以在一开始的时候先大步走，以提高搜寻效率，而一旦所达点的斜率逐渐下降，函数梯度下降的趋势会越来越缓，此时再逐步调小步距，以避免因步距过大而跨过谷底，从而最终找到最小值或近似最小值。

**至此，我们不难得出利用线性回归模型解决实际问题的一般步骤：**

- 根据问题构建一个线性模型，即构建一个函数；
- 利用已标注的样本数据对模型进行训练，训练过程中使用梯度下降法调整模型参数 β^i 、损失函数评价何时结束；
- 重复步骤2，直至找到损失函数的最小值；
- 利用验证集去测试模型的精度或拟合度，评价指标常用均方误差MSE(Mean Squared Error)；
- 如果预测结果不满意，则需改进模型(如加大训练集、改变学习率α等)；
- 回到第2步，再重新训练模型，直至获得满意的模型；
- 利用未来的自变量xi和满意模型，去计算预测值y，从而解决预测问题。





###  案例——预测中美两国GDP增长

#### 解决方案

 我们首先利用前面掌握的matplotlib知识，对GDP进行可视化分析，看走势是否符合线性回归的变化趋势，如果是，则采用机器学习提供的线性回归模型对数据进行训练，并测试模型的精度，当训练出合适的模型后，就相当于我们找到了一个明确的数学表达式，然后我们就可以利用它来预测未来GDP。解决方案如下图所示：

![](https://wx1.sinaimg.cn/large/008uF2zMgy1h5uql2tc3wj30qq07c75g.jpg)



#### 预备知识

1.**数据归一化**

用样本数据训练模型前，还需要对数据进行处理。有个别数据点偏离度较大，远离GDP趋势直线：归一化。

 归一化方法

（1）min-max标准化（Min-Max Normalization）       该方法也称为离差标准化，是对原始数据的线性变换，使结果值映射到[0-1]之间。转换公式如下：

![](https://wx4.sinaimg.cn/large/008uF2zMgy1h5uql35yvgj3056028mx1.jpg)

​    其中x^‘为归一化后的值，x_min为样本数据的最小值，x_max为样本数据的最大值。该方法有个缺陷就是当样本有新数据加入时，可能导致x_min和x_max的变化，导致需要重新计算归一化值。

（2）0均值标准化（Z-score standardization）	       这种方法基于原始数据的均值（mean）	和标准差（standard deviation）进行数据的	标准化。经过处理的数据符合标准正态分布，	即均值为0，标准差为1。转化公式为：

![](https://wx1.sinaimg.cn/large/008uF2zMgy1h5uql3y2z5j3039028t8i.jpg)

​    上式中的μ为所有样本数据的均值，σ为所有样本数据的标准差。



**2.线性回归模型如何训练**

  在准备好数据后，下一步就要构建模型。本案例的数据集只有年份和GDP两列，即线性模型中，只要一个自变量x（年份），模型的输出值就是GDP y。因此，模型的假设函数表达式是：

![](https://wx2.sinaimg.cn/large/008uF2zMgy1h5uql4c7z1j307m02mjrb.jpg)

   y^‘就是我们要预测的值，模型要学习的参数是(φ_0，φ_1)。建立好这个假设模型后，在后续模型学习过程中，需要给模型一个学习的目标，让它在学习过程中达到目标后即可停止学习，使得学习得到的参数(φ_0，φ_1)尽可能让预测值y^’接近真实值y。在此，我们使用前面提到的均方误差MSE，其表达式为：

![](https://wx4.sinaimg.cn/large/008uF2zMgy1h5uql4r4efj306k02swee.jpg)

​    有了模型的定义，就按照以下步骤对模型进行训练和测试：

(1)初始化参数，包括(φ_0，φ_1)、学习率∝和迭代次数n。

(2)将样本数据输入模型，计算损失函数Loss。

(3)利用学习优化算法如梯度下降法处理损失函数，寻找损失函数的最小值，并在此过程中依次更新模型的参数。

(4)不断重复步骤2、3，直到模型收敛于损失函数的最小值或训练迭代次数到达设定值n。



##### 任务1——可视化GDP数据

1.绘制散点图   

 先从数据集文件GDP.txt中读取数据，将一行数据的值(年份，GDP)作为一个点的坐标值(x,y)，然后用散点函数绘制出所有点的散点图。源代码如下:

```python
%matplotlib inline
import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt
df1=pd.read_csv('data\GDP.txt',sep=',',header=0)
plt.scatter(df1['year'],df1['USA'],c='b')
```

![](https://wx1.sinaimg.cn/large/008uF2zMgy1h5uql5hzptj30gq090q3m.jpg)

2.观察数据分布规律

​    由上图看出，整体样本数据呈现出一个线性分布的态势，因此，对于该问题，我们可以尝试使用线性回归模型来解决GDP预测的问题。



##### 任务2——线性回归模型的训练

  首先要构建一个线性回归模型，然后利用数据对它进行训练和测试，训练的目的就是让该模型拟合数据，让数据尽量分布在一条直线上。测试的目的就是要评估训练的效果如何，验证模型在测试集上是否能获得较好的性能。为此，利用根据任务目标，完成任务2。



线性回归模型的训练

1.数据的归一化处理

​    为消除不同量纲数据带来的影响，尽量提高模型训练精度，对GDP样本数据按照min-max标准化方法进行归一化处理。实现代码如下。	

`df=(df-df.min())/(df.max()-df.min())`    

经过上述代码处理后，df数据就缩放到[0-1]的范围，有利于后续模型的训练和测试。



2.产生训练集和测试集

​            在归一化数据后，为了更好地训练和评估模型，通常将数据分成两份，一份是用于训练模型的训练集，另一份是用于评估模型好坏的测试集。显然，当样本数据足够大时，两份数据集的比例可以为5:5，因为无论对于训练和测试而言，大量的数据都有利于模型参数调整和降低测试误差。本次样本数据只有41个，考虑到将更多的训练集用来训练模型，以得到更可信的模型，同时兼顾一定的测试误差，将训练集和测试集比例分配为8:2，代码如右。

```python
train_data=df.sample(frac=0.8,replace=False)
test_data=df.drop(train_data.index)
x_train=train_data['year'].values.reshape(-1, 1)
y_train=train_data['CHN'].values
x_test=test_data['year'].values.reshape(-1, 1)
y_test=test_data['CHN'].values

```

3.构建并训练模型

​    我们采用机器学习库scikit-learn中的随机梯度下降回归模型函数SGDRegressor来构建模型，并利用归一化后的数据对它进行训练。如果没有安装scikit-learn机器学习库，请在dos命令窗口下执行以下命令，来安装该第三方库。	pip3 install scikit-learn    安装好机器学习库scikit-learn后，编写如下代码，构建模型并对它进行训练。

```python
from  sklearn.linear_model import SGDRegressor
import joblib
model=SGDRegressor(max_iter=500,learning_rate=',eta0=0.1) 'constant
model.fit(x_train,y_train)
pre_score=model.score(x_train,y_train)
print('score=',pre_score)
print('coef=',model.coef_,'intercept=',model.intercept_)
joblib.dump(model,'.\sava_model\SGDRegressor.model')
```

##### 任务3——模型的测试及评价

​        在模型训练完成后，接下来要利用测试集对模型的训练效果进行评估，评估的基本思想是：      在模型上用测试集x_test计算出预测集y_pred；      计算测试集y_test和预测集y_pred两者之间的损失误差，此处采用均方差MSE作为评估模型的指标，来评估预测值和真实值之间的差距。      如果评估结果比较理想，就可以利用该模型进行GDP的预测；如果评估结果不够理想，可以对模型进行进一步优化，如增加训练集的数量，去除一些异常值，或者想办法增加训练样本的特征个数等。



1.计算均方差MSE

​	    在训练好的模型model上运用测试集x_test得到测试集y_pred；然后利用测试样本的真实值y_test和对应的预测集y_pred来计算损失MSE，以此作为模型的评价标准。实现代码如下。

```python
model=joblib.load('.\sava_model\SGDRegressor.model')
y_pred=model.predict(x_test)#得到预测值
print('测试集准确性得分=%.5f'%model.score(x_test,y_test))
#计算测试集的损失（用均方差）
MSE=np.mean((y_test - y_pred)**2)
print('损失MSE={:.5f}'.format(MSE))
```

执行上述代码，结果见下图所示。   

测试集准确得分：0.70548

损失 MSE ：0.00678

测试集的均方差约为0.007，预测的拟合得分为0.70548，说明GDP变动因素中约70%随着年份的变动来解释，拟合度还是不错的。



2.绘制预测效果图

  为直观了解预测效果，我们在真实样本散点图上绘制出预测直线，观察真实样本与预测直线的整体分布是否趋于一致，然后对比真实值与预测值的折线图的拟合情况如何。总之，当得到理想的模型后，就可以利用该模型对提供的自变量x预测出对应的y值，从而完成GDP的预测工作。绘图代码如下：

```python
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.figure(figsize=(10,4))
ax1=plt.subplot(121)
plt.scatter(x_test,y_test,label='测试集')
plt.plot(x_test,y_pred,'r',label='预测回归线')
ax1.set_xlabel('年份')
ax1.set_ylabel('GDP')
plt.legend(loc='upper left')
ax2=plt.subplot(122)
x=range(0,len(y_test))
plt.plot(x,y_test,'g',label='真实值')
plt.plot(x,y_pred,'r',label='预测值')
ax2.set_xlabel('样本序号')
ax2.set_ylabel('GDP')
plt.legend(loc='upper right')
```

