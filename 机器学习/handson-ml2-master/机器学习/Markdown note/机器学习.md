# 机器学习

# week 1

```python
import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
%matplotlib inline

#构建模拟数据
x=[i for i in range(1000)]   #特征
y=[15*x**2 + 3 + random.gauss(0,1) for x in x]

print(x)
print(y)

# 探索性分析
plt.scatter(x,y)

# 构建神经网络模型
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(1,input_shape=(1,)))
model.add(tf.keras.layers.Dense(1))
model.add(tf.keras.layers.Dense(1))
#模型编译
model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),
             loss = 'mse',
             metrics = ['mse'])

#可视化训练过程
plt.plot(history.epoch[20:],history.history.get('loss')[20:])
```

如果在 X 与 y 的关系中，如果是一次的关系，这个地方是构建一个神经网络模型

```python
# 构建神经网络模型
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(1,input_shape=(1,)))
model.add(tf.keras.layers.Dense(1))
```

如果是二次模型的话，就构建两个神经网络模型：

```python
# 构建神经网络模型
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(1,input_shape=(1,)))
model.add(tf.keras.layers.Dense(1))
model.add(tf.keras.layers.Dense(1))
```

![](https://wx4.sinaimg.cn/large/008uF2zMly8h5upotvuzvj30ac07774q.jpg)

![](https://wx1.sinaimg.cn/large/008uF2zMly8h5upotwqdwj30a2077jrk.jpg)

无监督学习相对监督学习而言，有以下特点：

- 无需大量的标注数据，从而减少大量的人力、物力和财力
- 以更加接近人类的学习方式不断自我发现、学习和调整，有利于发现世界的内在结果，找到新的模式或新的知识。

常见的无监督学习算法有聚类、关联分析等



# week 2 



万能逼近原理：MLP 能够逼近任何函数，如果逼近的效果不好，可以增加几个节点或者多加几层。



[绘制神经网络的网址](http://alexlenail.me/NN-SVG/)

[markdown grammar](https://markdown.com.cn/basic-syntax/links.html)



训练完之后保存模型：

```python
#训练模型，训练过程保存在history中
ck_pt_cb = tf.keras,callbacks.ModelCheckpoint("best_model.h5",save_best_only = True)
ck_pt_cb = tf.keras,callbacks.EarlyStopping(patience = 10,restore_best_weights = True )
```



```python
tf.keras.models.save_model(model,'hh.h5')
```

查看 `Tensorflow` 版本：`print('Tensorflow Version:{}'.format(tf.__version__))`


# week 3
损失函数：也叫代价函数，是线性回归常用来检测误差的指标。
代价函数是我们需要了解的：一个是假设函数，一个是代价函数
损失函数的计算：

![image](https://user-images.githubusercontent.com/109726121/189810495-06799dad-b671-4a07-b336-acde0356be6b.png)

![image](https://user-images.githubusercontent.com/109726121/189811131-e00efa7e-d788-464c-88b1-7b2e7dd035e5.png)

代价函数的形状：

![image](https://user-images.githubusercontent.com/109726121/189811449-382c32e3-7128-448b-96ac-5d3e1a8249df.png)


* 在课上老师说的 JO 函数在这里就是 J（θ_0,θ_1） *

## 梯度下降法
在下降的过程中找到局部最优解

![image](https://user-images.githubusercontent.com/109726121/189813263-b907b7c4-651e-46d7-9ce8-3c85075de062.png)

![image](https://user-images.githubusercontent.com/109726121/189813350-aa2e6e05-a476-474e-87a7-03b78f9a18d1.png)

在下面这张图中：

![image](https://user-images.githubusercontent.com/109726121/189813783-ea88ba53-e602-48e5-8d3d-3294d63ff6cc.png)

α：学习率（learning rate），控制我们在下降的时候迈出多大的步子
- α 值很大的时候，梯度下降就很快，步子迈的大，可能会错过局部最小，最后会在最小点来回地跨，最后偏离最小点
- α 值很小的时候，梯度下降就很慢，步子迈的小，需要迈很多步才可以到局部最小

θ0 ， θ1 需要同步更新才有意义

![无标题-2022-07-27-1119 excalidraw](https://user-images.githubusercontent.com/109726121/189818348-1ed60a70-e6aa-47d7-9fec-0417d82b8d1e.png)

局部最优点的倒数是 0 ，如果导数为 0 ，表示你已经到了局部最优点。
当我们快要接近局部最优点的时候，梯度下降法会自动采取更小的幅度，因为在我们接近局部最优点的时候，根据定义，在局部最低点时导数等于零，当我们接近最低点的时候，梯度会自动变小。




