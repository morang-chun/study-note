# 数据挖掘

# 工具准备



# 第一课

## 1.1 什么是数据挖掘

&emsp; 数据挖掘正是借助统计机器学习、深度学习等算法，从大量有噪声的、不完全的、模糊和随机的数据中，提取出隐含在其中的，人们事先不知道的、具有潜在利用价值的信息和知识的过程，从而实现判断和预测的一种技术。



 KDD （knowledge Discovery in Databases）数据库中的知识发现

数据挖掘是知识发现（KDD）的核心部分



## 1.2 数据挖掘算法

​	数据挖掘的算法主要有**传统机器学习算法**、**基于神经网络的深度学习算法**、**强化学习**以及**深化强化学习算法**等。

- **传统机器学习**：方法首先需要对原始数据做特征工程，提取有效的特征
- **深度学习**：不需要额外的特征工程，神经网络自主完成特征的提取
- **强化学习**：通过环境给出的奖惩来学习，不需要标签。强化学习是指决策的过程，通过过程模拟和观察来不断学习、通过奖惩与惩罚不断提高决策能力
- **深度强化学习**：是指运用了神经网络作为参数结构进行优化的强化学习算法



数据挖掘算法根据是否需要标签又为有监督学习算法、无监督学习算法

- **有监督学习**：指数据集中样本带有标签，有明确目标
- **无监督学习**：指数据集中的样本没有标签或者学习过程不借助数据集中的标签，典型的有 Meanshift、Kmeans的等聚类算法以及降维、关联规则挖掘等算法



## 1.3 数据挖掘目的

​	数据挖掘的主要目的是从已知的大量数据中发现潜在的规律。如通过数据挖掘掌握趋势和模式、做出预测以及求最优解等

​	不同的数据挖掘目的采用的算法不一样，回归、分类算法的目的是基于训练集数据建立数据的输入 x （特征）和输出 y （标签）之间的映射关系，也就是模型。区别在于分类算法的标签是离散的，而回归算法的标签是连续的。                   



**数据挖掘的过程：**

- 确定研究目标
- 获取数据（下载、爬取数据）
- 数据探索（初步了解数据）
- ==数据预处理==（缺失值、离群值、标准化、编码、离散化）
- 建模：要试一下各种方法，找到一个稍微好一点的模型
- 评价（返回、建模）
- 发布（OK）



课程论文：拿到别人没有用过的数据的时候，把这些东西弄出来，就可以发布论文了。



基本概念：

数据集：一组样本的集合

样本：数据集的一行，一个样本包含一个或多个特征，此外还可能包含一个标签

特征：在进行预测试时使用的输入变量



机器学习的基本方法

- 有监督学习
- 无监督学习
- 深度学习
- 强化学习（人工智能里面用的最多）



训练集：用于训练模型的数据集

测试集：用于测试模型的数据集

模型：建立数据的输入 X 和 输出 Y 之间的映射关系

损失函数：L(f(x~i~),y~i~) = (f(x~i~)-y~i~)^2^



**安装 jupyter 插件**

国内的常用的源有：

阿里云： http://mirrors.aliyun.com/pypi/simple/

中国科技大学： https://pypi.mirrors.ustc.edu.cn/simple/

豆瓣：http://pypi.douban.com/simple/

清华大学： https://pypi.tuna.tsinghua.edu.cn/simple/

中国科学技术大学： http://pypi.mirrors.ustc.edu.cn/simple/

https://www.lfd.uci.edu/~gohlke/pythonlibs

修改源方法（以下修改为清华大学源为例）： 

1） 

临时使用：

可以在使用 pip 的时候在后面加上-i 参数，指定 pip 源。

pip install pandas -i https://pypi.tuna.tsinghua.edu.cn/simple

从清华源下载安装 pandas 包，但每次安装都要指定源，比较麻烦

2） 

Windows 永久有效

在当前用户目录中创建一个 pip 目录，如：C:\Users\abc\pip（abc 为当前用户

名），在 pip 目录下新建文件 pip.ini，添加如下内容：

[global]

timeout = 6000

index-url = https://pypi.tuna.tsinghua.edu.cn/simple

trusted-host = pypi.tuna.tsinghua.edu.cn

或直接运行以下命令

pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

实现永久换源。



在命令提示符下输入：

```python
pip install jupyter_contrib_nbextensions

jupyter contrib nbextension install --user

pip install jupyter_nbextensions_configurator

jupyter nbextensions_configurator enable --user
```



重新运行 `jupyter notebook`，浏览器界面会多出选项卡“nbextensions”，将其打

开，按需选中对应的复选框

**常用快捷键**

Ctrl + Enter: 执行单元格代码

Shift + Enter: 执行单元格代码并且移动到下一个单元格

Alt + Enter: 执行单元格代码，新建并移动到下一个单元格

**命令模式常用命令**

| 命令                      | 作用                       |
| ------------------------- | -------------------------- |
| esc                       | 进入命令模式               |
| dd                        | 删除当前单元格             |
| shift + 向上箭头/向下箭头 | 同时选中上面或下面的单元格 |
| shift+m                   | 合并选中的单元格           |
| a                         | 当前单元格==前==增加单元格 |
| b                         | 当前单元格==后==增加单元格 |
| z                         | 撤回删除的单元格           |





# 第二课 数据预处理

&emsp; 原始的数据集往往因为样本的来源不同、计量单位不一致；采集设备抗干扰

能力差、人工输入的错误导致噪声数据；问卷填写不完整、采集设备故障导致的

数据缺失等数据质量问题.

在数 据挖掘过程中，数据预处理往往占大部分工作量。数据预处理通常包含==缺失值处理==、==异常值检测==与==处理、数据标准化、特征编码、离散化==以及==降维==等。

重要性：

- 模型数据输入质量直接影响建模效果
- 在正式构建模型之前需要对数据进行恰当的预处理

## 2.1 **缺失值处理**

 对于有缺失值的数据集，不能直接用于模型的训练，需要对缺失的数据进行删除或填充处理。

采用填补法可获得完整的数据集，对于连续型数据通常采用均值填充，离散型数据则采用众数或中位数填充。

采用删除法，根据具体情况删除样本（行）或特征（列）得到一个完整的数据子集。

### 2.1.1 **填补法**

​		对于连续型数据，使用平均值填充缺失值，如某居民信息收集过程中少数样本的收入缺失，数据处理时可以考虑用总体的均值填充，填补后得到完整的数据集。但要注意的是均值填补会使得相应特征的方差变小。

​		对于离散型数据，可以使用众数填充，如居民信息中少数样本的性别缺失，可以考虑使用数据表中多数样本的性别来填充。但这会加重特征取值的不平衡。



**解决方法：**

在 `sklearn.impute` 中提供 `SimpleImputer` 类实现缺失数据的填补。

```python
SimpleImputer(
missing_values=np.nan, #缺失值的占位符。
strategy=‘mean’, #填补策略
fill_value=None, #策略"constant"的常数
verbose=0, #控制 Imputer 的详细程度
copy=True #True，将创建 X 的副本；False，填补将在 X 上
进行，有例外
)
```

​		`missing_values` 参数指定缺失值的占位符，是指数据集中缺失值的表示方法，如果数据集中缺失值用“X”代替，`missing_values=“X”`，空通常其占位符用 np.nan（numpy 的属性）表示。

常见 sklearn 和 pandas 预处理模块

| 常见数据预处理方法 | 对于的 sklearn 和 pandas 的类                                |
| ------------------ | ------------------------------------------------------------ |
| 缺失值处理         | `sklearn.Impute.Simplemputer`                                |
| 离群值检测和处理   | `sklearn.neighbors.LocalOutlierFator`                        |
| 标准化             | `sklearn.preprocessing.StandarScaler` `sklearn.preprocessing.MinMaxScaler` `sklearn.preprocessing.RobustScaler` |
| 特征编码           | `sklearn.preprocessing.OneHotEncoder` `pandas.get_dummies` `sklearn.preprocessing.LabelEncoder` |
| 离散化             | `pandas.cut` `pandas.qcut` `sklearn.preprocessing.Binarizer` |

Strategy 参数确定填补策略：

- “mean”表示使用平均值填充缺失值
- “median”表示使用中位数填充缺失值
- “most_frequent”表示使用众数填充缺失值
- “constant”表示使用指定的常数填充缺失值，可用于离散型和连续型的特征。

对于部分特征如缺失较多可以考虑使用**单独的类别填充**，如性别缺失较多，但特征又不能删除，可以考虑用“中”或者其他某个字符代替缺失部分。策略为常数填充时，`fill_value`指定填充的值。

 

pandas 对象的方法 fillna来进行缺失值的处理：

`fillna(value=None,method=None,axis=None,inplace=False)`

- `method` 为填充的方法，取值有‘backfill’, ‘bfill’, ‘pad’, ‘ffill’, None, 缺省为None，
  - pad / ffill: 表示用缺失值前面的有效值填充。
  - backfill / bfill: 表示用缺失值后面的有效值填充。
- `axis` 指定填充时是==按列还是行==，0 表示沿行, 1 表示沿列。
- `inplace` 表示在**原始数据上修改，还是在副本上修改**。原始数据上修改则没有返回值，而副本上修改后会返回修改后的数据。
  - False 表示不在副本上修改
  - Ture 表示在副本上修改

**其他填补方法**

- 随机填补：是在均值填补的基础上加上随机项，通过增加缺失值的随机性来改善缺失值分布过于集中的缺陷。

- 基于模型的填补：将缺失特征当作预测目标，使用其余特征作为输入，利用缺失特征非缺失样本构建模型预测。

- 哑变量方法：对于离散型特征，将缺失值作为一个单独的取值进行处理。
- EM 算法填补：根据 EM (expectation and maximization)法则计算含有缺失值数据集的极大似然估计



### 2.2.2 **删除法**

>删除法通过删除包含缺失值的行或列，来得到一个完整的数据子集。主要通过删除特征或者样本来进行。

- **删除特征：**当某个特征缺失值较多，且该特征对数据分析的目标影响不大时，可以将该特征删除。

- **删除样本：**删除存在数据缺失的样本。该方法适合某些样本有多个特征存在缺失值，且存在缺失值的样本占整个数据集样本数量的比例不高的情形。

注意用来时间序列分析的数据，不能删除样本，删除样本会导致时间序列不连续，需要采用填补法。

```python
#pandas 对象的方法 dropna 可实现样本或特征的删除。
dropna(
axis=0,
how=‘any’,
thresh=None,
inplace=False
)
```

- how 通常用于特征的删除。

  - 当 how=“any”时，用 thresh 参数指定不被删除的特征的最少有效样本数。

  - how= 'all' 时，当某列或行数据全部缺失时删除该列或行。

- inplace 为 True 时直接修改原始数据，否则不修改原始数据，函数返回修改后的数据。

- axis 参数同样是表示沿行还是列

 

### 2.2.3 **标准化**

​		通常确定目标变量的特征会有多个，原始数据中不同特征的数值大小可能差异很大，数值大的特征对目标变量的影响程度将会比数值小的特征对目标变量的影响大，数值小的特征容易被模型忽略。而通过标准化处理，将不同特征的取值 都转换到相同的范围中，消除了特征之间数值量纲的差异。

#### **Z-Score** **标准化**

​		Z-Score 标准化首先计算特征的均值与标准差，然后对特征的每一个取值减去该特征的均值并除以其标准差，使得处理后的数据具有固定均值和标准差.

**𝑓~𝑖~′ = （𝑓~𝑖~−𝜇 ）/𝜎**

𝑓~𝑖~′为标准化后各数据点的取值

适用情况：Z-Score 标准化适用于特征的最大值或最小值未知、样本分布比较分散的情况。

```python
#可采用 sklearn.preprocessing 中的 StandardScaler 类来实现。
StandardScaler(
copy=True, # 为 False，尝试避免复制并改为直接替换
with_mean=False, # 为 True，则在缩放之前将数据居中
with_std=False # 为 True，则将数据缩放为单位方差（或单位标准差）
)
```

#### **Min-Max** **标准化**

Min-Max 标准化也称为离差标准化或最大-最小值标准化，通过对特征作线性变换，使得转换后特征的取值分布在 [0,1] 区间内
$$
𝑓𝑖′ = \frac{𝑓𝑖−𝑓𝑚𝑖𝑛}{𝑓𝑚𝑎𝑥-𝑓𝑚𝑖𝑛}
$$

$$
𝑓𝑖′ = a + (b-a)\frac{𝑓𝑖-𝑓𝑚𝑖𝑛}{𝑓𝑚𝑎𝑥-𝑓𝑚𝑖𝑛}
$$


```python
#可采用 sklearn.preprocessing 中的 MinMaxScaler 类来实现。
MinMaxScaler(
feature_range=(0, 1), #期望的转换后数据的取值范围
copy=False #设置为 False 以执行就地标准化并避免复制
)
```







